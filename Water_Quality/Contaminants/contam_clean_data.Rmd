---
title: "NDFA Contaminants Data: Cleaning"
author: "Dave Bosworth"
date: "8/31/2020"
output: 
  html_document: 
    code_folding: show
    toc: true
    toc_depth: 4
    toc_float: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Purpose

This document provides the code and decisions made to clean and standardize all contaminants data used for the North Delta Flow Action synthesis project. The data were collected by USGS, and includes concentration data for water, suspended sediment, and zooplankton samples. 

Jim Orlando from the USGS provided all of the contaminants data for 2016-2019 in a couple of Excel spreadsheets that are stored on the NDFA SharePoint site. This dataset provided by Jim Orlando did not include Method Detection Limits (MDL) for each parameter. To obtain the MDL values, we downloaded the contaminants data from the [USGS NWIS website](https://nwis.waterdata.usgs.gov/usa/nwis/qwdata) and added the MDL values to the data provided in the Excel spreadsheets.

# Global code and functions

```{r load packages, message = FALSE, warning = FALSE}
# Load packages
library(tidyverse)
library(readxl)
library(lubridate)
```

```{r load functions}
# Source global WQ functions
source("Water_Quality/global_wq_funcs.R")

# Source contaminants data cleaning functions
source("Water_Quality/Contaminants/contam_clean_data_funcs.R")
```

```{r define file paths}
# Define main NDFA file path for WQ subteam (assumes synced with SharePoint)
fp_fastr <- "California Department of Water Resources/Office of Water Quality and Estuarine Ecology - North Delta Flow Action/WQ_Subteam/"

# Define relative file paths for raw and processed contaminants data files
fp_rel_raw <- paste0(fp_fastr, "Raw_Data/Contaminants")
fp_rel_proc <- paste0(fp_fastr, "Processed_Data/Contaminants")

# Define absolute file paths
fp_abs_raw <- get_abs_path(fp_rel_raw)
fp_abs_proc <- get_abs_path(fp_rel_proc)

# Clean up
rm(fp_rel_raw, fp_rel_proc)
```

The data downloaded from NWIS has 5-digit numeric codes to represent each parameter measured. We developed a crosswalk table to convert these numeric codes to standardized names of the parameters. We also used this crosswalk table to standardize the parameter names in the Excel spreadsheets provided by Jim Orlando.

```{r import param code crosswalk}
# Bring in dataframe to be used as a crosswalk for the parameter codes and standardized names
param_code_cw <- 
  read_excel(
    paste0(fp_abs_raw, "/USGS_Data_Download_metadata.xlsx"), 
    sheet = "Parameter Codes"
  )
```

# Water Concentration Data

## Import Data

Import the water concentration data in the two Excel spreadsheets provided by Jim Orlando.

```{r import raw water data excel}
# Import water concentration data for 2016-2018:
water_conc16_18_orig <- 
  read_excel(
    path = paste0(fp_abs_raw, "/PesticideResults_YoloDWR_2016_18Data_ForDWR.xlsx"),
    sheet =  "Water",
    range = "B1:GB175",
    col_types = c(
      "text",
      rep("skip", 2),
      rep("date", 2),
      rep("skip", 3),
      rep("text", 175)
    )
  )
  
# Import water concentration data for 2019:
water_conc19_orig <- 
  read_excel(
    path = paste0(fp_abs_raw, "/2019YoloBypass_Pesticide Results_forDWR.xlsx"),
    sheet =  "Water",
    range = "C1:FS93",
    col_types = c(
      "text",
      rep("skip", 3),
      rep("date", 2),
      rep("skip", 3),
      rep("text", 164)
    )
  )
```

Import the MDL information downloaded from NWIS.

```{r import nwis water data mdl}
water_nwis_orig <-
  import_nwis_data(paste0(fp_abs_raw, "/CONTAM_RAW_NWIS_2015-2019.csv")) %>% 
  filter(medium_cd == "WS")
```

## Clean Data

### Clean up NA values

In both of the Excel spreadsheets provided by Jim Orlando, `NA` values represent measurements below the Method Detection Limit and "NA" values represent instances when the parameter wasn't analyzed. This needs to be cleaned up before proceeding with other data cleaning tasks.

```{r clean na values water}
# 2016-2018 data:
water_conc16_18_clean1 <- water_conc16_18_orig %>% 
  # first remove completely empty rows within the dataset
  filter(!is.na(Site)) %>%
  # use convert_na_val function to define NA values correctly
  convert_na_val(4, 178) %>% 
  # restructure dataframe and remove values that weren't analyzed
  pivot_longer(
    cols = -c(Site, Date, Time), 
    names_to = "Analyte", 
    values_to = "Result"
  ) %>% 
  filter(Result != "Not analyzed")

# 2019 data:
water_conc19_clean1 <- water_conc19_orig %>% 
  # first remove completely empty rows within the dataset
  filter(!is.na(Site)) %>%
  # use convert_na_val function to define NA values correctly
  convert_na_val(4, 167) %>% 
  # restructure dataframe and remove values that weren't analyzed
  pivot_longer(
    cols = -c(Site, Date, Time), 
    names_to = "Analyte", 
    values_to = "Result"
  ) %>% 
  filter(Result != "Not analyzed")
```

### Standardize Parameter names

Next we need to standardize the contaminant parameter names before combining all of the water concentration data.

```{r standard param names water}
# 2016-2018 data:
water_conc16_18_clean2 <- water_conc16_18_clean1 %>% 
  left_join(param_code_cw, by = c("Analyte" = "parameter_16_18_xlsx")) %>% 
  select(
    Site,
    Date,
    Time,
    Analyte = parameter_std,
    PesticideType = pesticide_type,
    Result
  ) %>% 
  # remove parameters we aren't keeping in the dataset (Clothianidin des methyl)
  filter(Analyte != "remove")

# 2019 data:
water_conc19_clean2 <- water_conc19_clean1 %>% 
  left_join(param_code_cw, by = c("Analyte" = "parameter_19_xlsx")) %>% 
  select(
    Site,
    Date,
    Time,
    Analyte = parameter_std,
    PesticideType = pesticide_type,
    Result
  ) %>% 
  # remove parameters we aren't keeping in the dataset (Clothianidin des methyl and Imidacloprid desnitro)
  filter(Analyte != "remove")
```

### Combine data

The contaminant water concentration data from 2016-2018 and 2019 can now be combined into one dataframe. We will also need to standardize the "Site" variable and convert the "Date" variable to a date object so that the MDL values from the data downloaded from NWIS can be joined correctly.

```{r combine water data}
water_conc_all1 <- 
  bind_rows(water_conc16_18_clean2, water_conc19_clean2) %>% 
  mutate(
    Site = case_when(
      Site == "I-80" ~ "I80",
      Site == "RY1" ~ "RYI",
      TRUE ~ Site
    ),
    Date = as_date(Date)
  )
```

### Add MDL values

We need to add the Method Detection Limit (MDL) values downloaded from NWIS to the main dataset. The contaminant data downloaded from NWIS has 5-digit numeric codes to represent each parameter measured. We need to convert these numeric codes to the standardized parameter names so that this data can be joined to the main dataset correctly. We will also convert the "sample_dt" variable to a date object so that the join can occur correctly.

```{r mod nwis water data mdl}
water_nwis_clean <- water_nwis_orig %>% 
  left_join(param_code_cw, by = c("parm_cd" = "parameter_code")) %>% 
  mutate(sample_dt = ymd(sample_dt)) %>% 
  select(
    StationCode,
    sample_dt,
    parameter_std,
    remark_cd,
    result_va,
    rpt_lev_va,
    result_lab_cm_tx
  )
```

We can now join the NWIS data with the MDL values to the main dataset.

```{r join nwis water data mdl}
water_conc_all2 <- water_conc_all1 %>% 
  left_join(
    water_nwis_clean,
    by = c(
      "Site" = "StationCode",
      "Date" = "sample_dt",
      "Analyte" = "parameter_std"
    )
  )
```

Check to see if all of the rows in the main dataset had corresponding records in the NWIS dataset.

```{r check missing mdl val water}
anyNA(water_conc_all2$rpt_lev_va)
```

Not all of the rows in the main dataset had corresponding records in the NWIS dataset. Therefore, we will need to estimate the MDL values for these missing records. First, we need to summarize the MDL values for each parameter measured to determine how consistent they were throughout the period of record.

```{r summarize mdl val water}
# Summarize the MDL values for each parameter
water_mdl_val <- water_nwis_clean %>% 
  count(parameter_std, rpt_lev_va)

# Determine if any parameters had more than one MDL value during the period of record
water_mdl_val %>% count(parameter_std) %>% filter(n > 1)
```

Most of the parameters had the same MDL value throughout the period of record. However, 14 parameters had 2 MDL values during the period of record. We need to look at these 14 parameters more closely to decide which MDL value to assign to these parameters.

```{r look at duplicate mdl val water}
# Create a vector of the parameters with 2 MDL values during the period of record
water_mdl_val_dup <- water_mdl_val %>% 
  count(parameter_std) %>% 
  filter(n > 1) %>% 
  pull(parameter_std)

# Filter out these 14 parameters to take a closer look at them
water_mdl_val %>% filter(parameter_std %in% water_mdl_val_dup)
```

All of the parameters with 2 MDL values had one MDL value that was very frequent and another that was rare. We'll use the frequent MDL values to estimate the MDL values for these 14 parameters.

```{r create df for estimated mdl val water}
# Keep the frequent MDL value for the 14 parameters with 2 MDL values
water_mdl_val2 <- water_mdl_val %>% 
  filter(
    parameter_std %in% water_mdl_val_dup,
    n > 7
  )

# Create a dataframe to use to estimate the MDL values of each parameter for the water concentration data with missing MDL values
water_mdl_val_f <- water_mdl_val %>% 
  filter(!parameter_std %in% water_mdl_val_dup) %>% 
  bind_rows(water_mdl_val2) %>% 
  select(-n)
```

We can now use the `water_mdl_val_f` dataframe to estimate MDL values for the records in the main dataset without a provided MDL.

```{r estimate missing mdl val water}
# Provide estimated MDL values for the records without a MDL in the main dataframe
water_conc_all2_estMDL <- water_conc_all2 %>% 
  filter(is.na(rpt_lev_va)) %>% 
  select(-rpt_lev_va) %>% 
  left_join(water_mdl_val_f, by = c("Analyte" = "parameter_std"))

# Add the records with estimated MDL values back to the main dataframe
water_conc_all3 <- water_conc_all2 %>% 
  filter(!is.na(rpt_lev_va)) %>% 
  bind_rows(water_conc_all2_estMDL)
```

### Finish Cleaning data

We'll finish cleaning the dataframe with the 2016-2019 water concentrations by:

* Parsing the Time variable to a time object
* Creating a new variable to identify estimated values (reported values below the MDL) and values above and below the Method Detection Limit (MDL)
* Creating a new variable for the measurement units
* Making the Result variable equal to its MDL value for the samples below the MDL
* Rounding the Result variable to 2 significant figures for values less than 10 and 3 significant figures for values greater than 10
* Changing some variable names to standardized names for the NDFA synthesis

```{r finish cleaning water df, warning = FALSE}
water_conc_all_f <- water_conc_all3 %>% 
  mutate(
    # parse Time variable to a time object
    Time = hms::as_hms(Time),
    # create a new variable to identify values above and below the MDL
    LabDetect = if_else(Result == "Non-detect", "Non-detect", "Detect"),
    # convert result variable to numeric- use MDL values for <MDL data
    Result = if_else(LabDetect == "Non-detect", rpt_lev_va, as.numeric(Result)),
    # Round the Result variable to the appropriate number of significant figures
    Result = if_else(Result < 10, signif(Result, 2), signif(Result, 3)),
    # add "Estimated" category to the LabDetect variable for reported values less than the MDL
    LabDetect = if_else(Result < rpt_lev_va, "Estimated", LabDetect),
    # create a variable for measurement units
    Units = "ng/L"
  ) %>% 
  # standardize variable names
  select(
    StationCode = Site,
    Date,
    Time,
    Analyte,
    PesticideType,
    Result,
    LabDetect,
    MDL = rpt_lev_va,
    Units
  )
```

## Export Data

Export the contaminant water concentration data as a .csv file to the Processed_Data/Contaminants folder on SharePoint.

```{r export water data as csv, eval = FALSE}
# Code chunk is set to eval = FALSE, so this code is not executed when this file is knitted
# change eval option to TRUE when you want to export the data when knitting this file
water_conc_all_f %>% 
  write_excel_csv(
    path = paste0(fp_abs_proc, "/WQ_OUTPUT_Contam_water_formatted.csv"),
    na = ""
  )
```

Clean up objects in the global environment to only keep necessary objects at this point.

```{r clean obj from global env water}
# Remove all objects related to cleaning the water concentration data
rm(list = ls()[str_detect(ls(), "^water")])
```


# Suspended Sediment Concentration Data

## Import Data

Import the suspended sediment concentration data in the two Excel spreadsheets provided by Jim Orlando.

```{r import raw susp sed data excel}
# Import suspended sediment concentration data for 2016-2018:
ss_conc16_18_orig <- 
  read_excel(
    path = paste0(fp_abs_raw, "/PesticideResults_YoloDWR_2016_18Data_ForDWR.xlsx"),
    sheet =  "Suspended Sed",
    range = "B1:EU175",
    col_types = c(
      "text",
      rep("skip", 2),
      rep("date", 2),
      rep("skip", 2),
      rep("numeric", 2),
      rep("text", 141)
    )
  )
  
# Import suspended sediment concentration data for 2019:
ss_conc19_orig <- 
  read_excel(
    path = paste0(fp_abs_raw, "/2019YoloBypass_Pesticide Results_forDWR.xlsx"),
    sheet =  "Suspended Sediment",
    range = "C1:EJ93",
    col_types = c(
      "text",
      rep("skip", 3),
      rep("date", 2),
      rep("skip", 2),
      rep("numeric", 2),
      "skip",
      rep("text", 127)
    )
  )
```

Import the MDL information downloaded from NWIS.

```{r import nwis susp sed data mdl}
ss_nwis_orig <-
  import_nwis_data(paste0(fp_abs_raw, "/CONTAM_RAW_NWIS_2015-2019.csv")) %>% 
  filter(medium_cd == "SS")
```

## Clean Data

## Export Data


# Zooplankton Concentration Data

## Import Data

## Clean Data

## Export Data





